{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training script useful for debugging UDify and AllenNLP code\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import datetime\n",
        "import logging\n",
        "import argparse\n",
        "import glob\n",
        "\n",
        "from allennlp.common import Params\n",
        "from allennlp.common.util import import_submodules\n",
        "from allennlp.commands.train import train_model\n",
        "\n",
        "from udify import util\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--name\", default=\"\", type=str, help=\"Log dir name\")\n",
        "parser.add_argument(\"--base_config\", default=\"config/udify_base.json\", type=str, help=\"Base configuration file\")\n",
        "parser.add_argument(\"--config\", default=[], type=str, nargs=\"+\", help=\"Overriding configuration files\")\n",
        "parser.add_argument(\"--dataset_dir\", default=\"data/ud-treebanks-v2.5\", type=str, help=\"The path containing all UD treebanks\")\n",
        "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"The batch size used by the model; the number of training sentences is divided by this number.\")\n",
        "parser.add_argument(\"--device\", default=None, type=int, help=\"CUDA device; set to -1 for CPU\")\n",
        "parser.add_argument(\"--resume\", type=str, help=\"Resume training with the given model\")\n",
        "parser.add_argument(\"--lazy\", default=None, action=\"store_true\", help=\"Lazy load the dataset\")\n",
        "parser.add_argument(\"--cleanup_archive\", action=\"store_true\", help=\"Delete the model archive\")\n",
        "parser.add_argument(\"--replace_vocab\", action=\"store_true\", help=\"Create a new vocab and replace the cached one\")\n",
        "parser.add_argument(\"--archive_bert\", action=\"store_true\", help=\"Archives the finetuned BERT model after training\")\n",
        "parser.add_argument(\"--predictor\", default=\"udify_predictor\", type=str, help=\"The type of predictor to use\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "log_dir_name = args.name\n",
        "if not log_dir_name:\n",
        "    file_name = args.config[0] if args.config else args.base_config\n",
        "    log_dir_name = os.path.basename(file_name).split(\".\")[0]\n",
        "\n",
        "if not args.name == \"multilingual\":\n",
        "    train_file = args.name + \"-ud-train.conllu\"\n",
        "    pathname = os.path.join(args.dataset_dir, \"*\", train_file)\n",
        "    train_path = glob.glob(pathname).pop()\n",
        "    treebank_path = os.path.dirname(train_path)\n",
        "\n",
        "    if train_path:\n",
        "        logger.info(f\"found training file: {train_path}, calculating the warmup and start steps\")\n",
        "    \n",
        "        f = open(train_path, 'r', encoding=\"utf-8\")\n",
        "        sentence_count = 0\n",
        "        for line in f.readlines():\n",
        "            if line.isspace():\n",
        "                sentence_count += 1\n",
        "        num_warmup_steps = round(sentence_count / args.batch_size)\n",
        "\n",
        "configs = []\n",
        "\n",
        "if not args.resume:\n",
        "    serialization_dir = os.path.join(\"logs\", log_dir_name, datetime.datetime.now().strftime(\"%Y.%m.%d_%H.%M.%S\"))\n",
        "\n",
        "    overrides = {}\n",
        "    if args.device is not None:\n",
        "        overrides[\"trainer\"] = {\"cuda_device\": args.device}\n",
        "    if args.lazy is not None:\n",
        "        overrides[\"dataset_reader\"] = {\"lazy\": args.lazy}\n",
        "    configs.append(Params(overrides))\n",
        "    for config_file in args.config:\n",
        "        configs.append(Params.from_file(config_file))\n",
        "    configs.append(Params.from_file(args.base_config))\n",
        "else:\n",
        "    serialization_dir = args.resume\n",
        "    configs.append(Params.from_file(os.path.join(serialization_dir, \"config.json\")))\n",
        "\n",
        "train_params = util.merge_configs(configs)\n",
        "\n",
        "if not args.name == \"multilingual\":\n",
        "    # overwrite the default params with the language-specific ones\n",
        "    for param in train_params:\n",
        "        if param == \"train_data_path\":\n",
        "            train_params[\"train_data_path\"] = os.path.join(treebank_path, f\"{args.name}-ud-train.conllu\")\n",
        "        if param == \"validation_data_path\":\n",
        "            train_params[\"validation_data_path\"] = os.path.join(treebank_path, f\"{args.name}-ud-dev.conllu\")\n",
        "        if param == \"test_data_path\":\n",
        "            train_params[\"test_data_path\"] = os.path.join(treebank_path, f\"{args.name}-ud-test.conllu\")\n",
        "        \n",
        "        if param == \"vocabulary\":\n",
        "            train_params[\"vocabulary\"][\"directory_path\"] = f\"data/vocab/{args.name}/vocabulary\"\n",
        "        \n",
        "        if param == \"trainer\":\n",
        "            for sub_param in train_params[\"trainer\"]:\n",
        "                if sub_param == \"learning_rate_scheduler\":\n",
        "                    train_params[\"trainer\"][\"learning_rate_scheduler\"][\"warmup_steps\"] = num_warmup_steps\n",
        "                    train_params[\"trainer\"][\"learning_rate_scheduler\"][\"start_step\"] = num_warmup_steps\n",
        "                    \n",
        "                    logger.info(f\"changing warmup and start steps for {train_path} to {num_warmup_steps}\")\n",
        "                \n",
        "if \"vocabulary\" in train_params:\n",
        "    # Remove this key to make AllenNLP happy\n",
        "    train_params[\"vocabulary\"].pop(\"non_padded_namespaces\", None)\n",
        "\n",
        "predict_params = train_params.duplicate()\n",
        "\n",
        "import_submodules(\"udify\")\n",
        "\n",
        "try:\n",
        "    util.cache_vocab(train_params)\n",
        "    train_model(train_params, serialization_dir, recover=bool(args.resume))\n",
        "except KeyboardInterrupt:\n",
        "    logger.warning(\"KeyboardInterrupt, skipping training\")\n",
        "\n",
        "dev_file = predict_params[\"validation_data_path\"]\n",
        "test_file = predict_params[\"test_data_path\"]\n",
        "\n",
        "dev_pred, dev_eval, test_pred, test_eval = [\n",
        "    os.path.join(serialization_dir, name)\n",
        "    for name in [\"dev.conllu\", \"dev_results.json\", \"test.conllu\", \"test_results.json\"]\n",
        "]\n",
        "\n",
        "if dev_file != test_file:\n",
        "    util.predict_and_evaluate_model(args.predictor, predict_params, serialization_dir, dev_file, dev_pred, dev_eval)\n",
        "\n",
        "util.predict_and_evaluate_model(args.predictor, predict_params, serialization_dir, test_file, test_pred, test_eval)\n",
        "\n",
        "if args.archive_bert:\n",
        "    bert_config = \"config/archive/bert-base-multilingual-cased/bert_config.json\"\n",
        "    util.archive_bert_model(serialization_dir, bert_config)\n",
        "\n",
        "util.cleanup_training(serialization_dir, keep_archive=not args.cleanup_archive)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}